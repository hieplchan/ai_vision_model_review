{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)**    \n",
    "As we showed before, keeping the standard deviation of layers’ activations around 1 will **allow us to stack several more layers** in a deep neural network without gradients exploding or vanishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_dim = 28*28 # 28 pixels image\n",
    "num_hidden = 784\n",
    "inp_dim, num_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate 100 layers depth network without activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights is too big -> exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(nan), tensor(nan))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(inp_dim)\n",
    "w = torch.randn(inp_dim, num_hidden)\n",
    "x.shape, w.shape\n",
    "for i in range(100):\n",
    "    x = w @ x\n",
    "x.std(), x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break after 26 multiplications\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(inp_dim)\n",
    "w = torch.randn(inp_dim, num_hidden)\n",
    "x.shape, w.shape\n",
    "for i in range(100):\n",
    "    x = w @ x\n",
    "    if x.std() != x.std():\n",
    "        print('Break after {} multiplications'.format(i))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights is too small -> vanishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(inp_dim)\n",
    "w = torch.randn(inp_dim, num_hidden)*0.01 # weights init with std=0.01\n",
    "x.shape, w.shape\n",
    "for i in range(100):\n",
    "    x = w @ x\n",
    "x.std(), x.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore sweet spot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## element wise calculate explained - home grown method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0564), tensor(0.9571))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, variance = 0., 0.\n",
    "for i in range(100):\n",
    "    x = torch.randn(1)\n",
    "    w = torch.randn(1)\n",
    "    y = w @ x\n",
    "    mean += y\n",
    "    variance += y**2\n",
    "mean/100, variance/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to calculate `y` we sum `inp_dim` products of the element-wise multiplication of one row of the weights `w` by one element of the inputs `x`  \n",
    "each product have mean 0 and std 1 so total variance is `inp_dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1593), tensor(791.2849))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, variance = 0., 0.\n",
    "for i in range(100):\n",
    "    x = torch.randn(inp_dim)\n",
    "    w = torch.randn(num_hidden, inp_dim)\n",
    "    y = w @ x\n",
    "    mean += y.mean()\n",
    "    variance += y.pow(2).mean()\n",
    "mean/100, variance/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some method used before 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**no activations**   \n",
    "- supposed to be linear (just matmul)  \n",
    "- good result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0017), tensor(0.6494))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, variance = 0., 0.\n",
    "x = torch.randn(inp_dim)\n",
    "for i in range(100):\n",
    "    w = torch.randn(num_hidden, inp_dim)/math.sqrt(inp_dim)\n",
    "    x = w @ x\n",
    "x.mean(), x.std() # good mean and std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**non-linear activations**\n",
    "- bad result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with some common non-linear activations\n",
    "def tanh(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0025), tensor(0.0533))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(inp_dim)\n",
    "for i in range(100):\n",
    "    w = torch.randn(num_hidden, inp_dim)/math.sqrt(inp_dim)\n",
    "    x = tanh(w @ x)\n",
    "#     x = sigmoid(w @ x)\n",
    "#     x = relu(w @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**standard init approach before 2010**\n",
    "- initializing weights from a uniform distribution in [-1,1] and then scaling by 1/√n\n",
    "- bad result, just like vanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.9219e-26), tensor(1.0964e-24))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(inp_dim)\n",
    "for i in range(100):\n",
    "    w = torch.Tensor(num_hidden, inp_dim).uniform_(-1, 1) * math.sqrt(1./inp_dim)\n",
    "    x = tanh(w @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier (normalized) initialization \n",
    "[Landmark Origin Paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?source=post_page---------------------------)\n",
    "- Identical to home-grown method\n",
    "![Formula](https://miro.medium.com/max/875/1*H6t3yYBLlinNRUwmL-d7vw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_uniform_init(fan_in, fan_out):\n",
    "    return torch.Tensor(fan_in, fan_out).uniform_(-1, 1) * math.sqrt(6./(fan_in + fan_out))\n",
    "\n",
    "def xavier_normal_init(fan_in, fan_out):\n",
    "    return torch.Tensor(fan_in, fan_out).normal_() * math.sqrt(2./(fan_in + fan_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0007), tensor(0.0585))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with some Xavier method\n",
    "x = torch.randn(inp_dim)\n",
    "for i in range(100):\n",
    "    w = xavier_uniform_init(inp_dim, num_hidden)\n",
    "#     w = xavier_normal_init(inp_dim, num_hidden)\n",
    "#     w = torch.nn.init.xavier_uniform_(torch.FloatTensor(inp_dim, num_hidden))\n",
    "#     w = torch.nn.init.xavier_normal_(torch.FloatTensor(inp_dim, num_hidden))\n",
    "    x = tanh(w @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaiming initialization\n",
    "What if activation functions **asymmetric, non-linear activations** such as: **ReLU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU inspection \n",
    "- variance~inp_dim/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return torch.clamp(x, min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(11.1279), tensor(389.5249))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variance~784/2\n",
    "mean, variance = 0., 0.\n",
    "for i in range(100):\n",
    "    x = torch.randn(inp_dim)\n",
    "    w = torch.randn(num_hidden, inp_dim)\n",
    "    y = relu(w @ x)\n",
    "    mean += y.mean()\n",
    "    variance += y.pow(2).mean()\n",
    "mean/100, variance/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to use home-grown init method\n",
    "- std ~ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5628), tensor(0.9925))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, variance = 0., 0.\n",
    "for i in range(100):\n",
    "    x = torch.randn(inp_dim)\n",
    "    w = torch.randn(num_hidden, inp_dim)*math.sqrt(2./inp_dim)\n",
    "    y = relu(w @ x)\n",
    "    mean += y.mean()\n",
    "    variance += y.pow(2).mean()\n",
    "mean/100, variance/100    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their 2015 paper, He et. al. demonstrated that deep networks (e.g. a 22-layer CNN) would **converge much earlier** if the following input weight initialization strategy is employed:  \n",
    "- Create a tensor with the dimensions appropriate for a weight matrix at a given layer, and populate it with numbers randomly chosen from a **standard normal distribution**.  \n",
    "- Multiply each randomly chosen number by `√2/√n` where `n` is the number of **incoming connections** coming into a given layer from the previous layer’s output (also known as the **“fan-in”**).  \n",
    "- **Bias** tensors are **initialized to zero**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaiming He vs Xavier on ReLU\n",
    "- Xavier init: vanished in 100 layer depth\n",
    "- Kaiming init: just good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(inp_dim, num_hidden):\n",
    "    return torch.randn(inp_dim, num_hidden)*math.sqrt(2./inp_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5421), tensor(0.7700))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kaiming He init\n",
    "x = torch.randn(inp_dim)\n",
    "\n",
    "for i in range(100):\n",
    "    w = kaiming_init(inp_dim, num_hidden)\n",
    "    x = relu(w @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7.1177e-16), tensor(1.0367e-15))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xavier init -> vanished\n",
    "x = torch.randn(inp_dim)\n",
    "\n",
    "for i in range(100):\n",
    "    w = xavier_normal_init(inp_dim, num_hidden)\n",
    "    x = relu(w @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
